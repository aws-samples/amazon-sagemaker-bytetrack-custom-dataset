{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ByteTrack Inference with Amazon SageMaker\n",
    "\n",
    "This notebook will demonstrate how to create an endpoint for real time inference with the trained FairMOT model. We will first deploy the trained model in Sagemaker using [BYOS](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-script-mode/sagemaker-script-mode.html) mode by using custom inference scripts. And then apply inference on each frame of the video by invoking the endpoint. The inference result will be saved to a local directory.\n",
    "SageMaker provided prebuilt containers for various frameworks like Scikit-learn, PyTorch, and XGBoost. For this example, we will use PyTorch prebuilt containers by defining a PyTorchModel instance. \n",
    "\n",
    "## 1. SageMaker Initialization \n",
    "First we upgrade SageMaker to the latest version. If your notebook is already using latest Sagemaker 2.x API, you may skip the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade pip\n",
    "! python3 -m pip install --upgrade sagemaker\n",
    "! pip install cython_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and get execution role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "print(f'AWS account:{account}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "aws_region = session.region_name\n",
    "print(f\"AWS region:{aws_region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploy YOLOX model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to complete training job on [bytetrack-training.ipynb](bytetrack-training.ipynb) before running the following steps. Script Mode in SageMaker allows you to take control of the training and inference process without having to go through the trouble of creating and maintaining your own docker containers. Here, we want to use a pytorch algorithm, just use the AWS-provided Pytorch container and pass our own inference code. On your behalf, the SageMaker Python SDK will package this entry point script (sagemaker-serving/code/inferece.py), upload it to S3, and set two environment variables that are read at runtime and load the custom inference functions from the entry point script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the s3 path for the model trained in [bytetrack-training.ipynb](bytetrack-training.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r s3_model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside inference.py, we defined 4 functions: model_fn, input_fn, predict_fn, and output_fn. These function handlers are automatically loaded and executed at runtime. The argument variables for these function handlers are predefined by SageMaker prebuilt containers. The model_fn handler loads the model according to s3 path, while the input_fn handler defines steps to pre-process the image passed by the requests. The predict_fn handler defines the model forward computing steps, and finally the output_fn handler defines the post-processing steps after getting the inference results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchModel(\n",
    "    model_data=s3_model_uri,\n",
    "    role=role,\n",
    "    source_dir=\"sagemaker-serving/code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this cell, you would need to define the endpoint name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = <endpint name>\n",
    "pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Multi-Object Tracking with YOLOX\n",
    "\n",
    "We deploy YOLOX model into SageMaker endpoint, and run tracking task in client side. We will use the tracking scripts provided [ByteTrack](https://github.com/ifzhang/ByteTrack/tree/main/yolox/tracker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile download_tracking.sh\n",
    "git clone --filter=blob:none --no-checkout --depth 1 --sparse https://github.com/ifzhang/ByteTrack.git && \\\n",
    "cd ByteTrack && \\\n",
    "git sparse-checkout set yolox && \\\n",
    "git checkout && \\\n",
    "cd ..\n",
    "cp -r ByteTrack/yolox yolox\n",
    "cp container-batch-inference/byte_tracker.py yolox/tracker/\n",
    "sudo rm -r ByteTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash download_tracking.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install lap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines a function which captures each frame of a video, passes each frame to the inference endpoint defined in the previous step and saves the resulting frames to a local directory (save_folder). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolox.tracker.byte_tracker import BYTETracker\n",
    "import cv2\n",
    "import time\n",
    "from yolox.tracking_utils.timer import Timer\n",
    "import os.path as osp\n",
    "import os\n",
    "import torch\n",
    "from yolox.utils.visualize import plot_tracking\n",
    "\n",
    "sm_runtime = boto3.Session().client(\"sagemaker-runtime\")\n",
    "\n",
    "def imageflow_demo(endpoint_name, video_path=\"\", save_folder=\"\"):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # float\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # float\n",
    "    \n",
    "    print(f\"width: {width}, height: {height}\")\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    save_path = osp.join(save_folder, video_path.split(\"/\")[-1])\n",
    "    \n",
    "    print(f\"video save_path is {save_path}\")\n",
    "    vid_writer = cv2.VideoWriter(\n",
    "        save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (int(width), int(height))\n",
    "    )\n",
    "    \n",
    "    aspect_ratio_thresh = 1.6\n",
    "    min_box_area = 10\n",
    "    tracker = BYTETracker(\n",
    "        frame_rate=30,\n",
    "        track_thresh=0.5,\n",
    "        track_buffer=30,\n",
    "        mot20=False,\n",
    "        match_thresh=0.8\n",
    "    )\n",
    "    timer = Timer()\n",
    "    frame_id = 0\n",
    "    results = []\n",
    "    while True:\n",
    "        ret_val, frame = cap.read()\n",
    "        if ret_val:\n",
    "            cv2.imwrite(f'datasets/frame_{frame_id}.png', frame)\n",
    "            with open(f\"datasets/frame_{frame_id}.png\", \"rb\") as f:\n",
    "                payload = f.read()\n",
    "            \n",
    "            timer.tic()\n",
    "            response = sm_runtime.invoke_endpoint(\n",
    "                EndpointName=endpoint_name, ContentType=\"application/x-image\", Body=payload\n",
    "            )\n",
    "            outputs = json.loads(response[\"Body\"].read().decode())\n",
    "            \n",
    "            if outputs[0] is not None:\n",
    "                online_targets = tracker.update(torch.as_tensor(outputs[0]), [height, width], (800, 1440))\n",
    "                online_tlwhs = []\n",
    "                online_ids = []\n",
    "                online_scores = []\n",
    "                for t in online_targets:\n",
    "                    tlwh = t.tlwh\n",
    "                    tid = t.track_id\n",
    "                    vertical = tlwh[2] / tlwh[3] > aspect_ratio_thresh\n",
    "                    if tlwh[2] * tlwh[3] > min_box_area and not vertical:\n",
    "                        online_tlwhs.append(tlwh)\n",
    "                        online_ids.append(tid)\n",
    "                        online_scores.append(t.score)\n",
    "                        results.append(\n",
    "                            f\"{frame_id},{tid},{tlwh[0]:.2f},{tlwh[1]:.2f},{tlwh[2]:.2f},{tlwh[3]:.2f},{t.score:.2f},-1,-1,-1\\n\"\n",
    "                        )\n",
    "                timer.toc()\n",
    "                online_im = plot_tracking(\n",
    "                    frame, online_tlwhs, online_ids, frame_id=frame_id + 1, fps=1. / timer.average_time\n",
    "                )\n",
    "            else:\n",
    "                timer.toc()\n",
    "                online_im = frame\n",
    "            if frame_id % 20 == 0:\n",
    "                print('Processing frame {} ({:.2f} fps)'.format(frame_id, 1. / max(1e-5, timer.average_time)))\n",
    "            \n",
    "            vid_writer.write(online_im)\n",
    "        else:\n",
    "            break\n",
    "        frame_id += 1\n",
    "\n",
    "    res_file = osp.join(save_folder, f\"log.txt\")\n",
    "    with open(res_file, 'w') as f:\n",
    "        f.writelines(results)\n",
    "    print(f\"save results to {res_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a video from a public resource. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets\n",
    "!wget https://raw.githubusercontent.com/ifzhang/FairMOT/master/videos/MOT16-03.mp4 -O datasets/MOT16-03.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_path=\"datasets/MOT16-03.mp4\"\n",
    "save_folder=\"track_res\"\n",
    "imageflow_demo(endpoint_name, video_path, save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Inference speed comparison\n",
    "\n",
    "<table>\n",
    "    <tr><th>Instance</th><th>FPS</th></tr>\n",
    "    <tr><td>ml.g4dn.2xlarge</td><td>3.6</td></tr>\n",
    "    <tr><td>ml.p3.2xlarge</td><td>5</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
