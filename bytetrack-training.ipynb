{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ByteTrack Training with Amazon SageMaker\n",
    "\n",
    "This notebook demonstrates how to train a [ByteTrack](https://arxiv.org/abs/2110.06864) model with SageMaker and tune hyper-parameters with [SageMaker Hyperparameter tuning job](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html). We will use the dataset labeled by SageMaker Ground Truth in [data-preparation.ipynb](data-preparation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SageMaker Initialization \n",
    "First we upgrade SageMaker to the latest version. If your notebook is already using the latest SageMaker 2.x API, you may skip the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade pip\n",
    "! python3 -m pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "role = get_execution_role() # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f'SageMaker Execution Role:{role}')\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "print(f'AWS account:{account}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f'AWS region:{region}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = <your-s3-bucket-name> # your-s3-bucket-name\n",
    "\n",
    "dataset_name= <your-dataset-name> # dataset name\n",
    "\n",
    "prefix = <prefix>\n",
    "\n",
    "training_image = f\"{account}.dkr.ecr.{region}.amazonaws.com/bytetrack-sagemaker:pytorch1.12.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset\n",
    "\n",
    "We prepare our dataset as below:\n",
    "- Convert SageMaker Ground Truth annotation into MOT Challenge annotation\n",
    "- Convert MOT annotation into MSCOCO annotation\n",
    "\n",
    "As we keep both MOT format dataset and MSCOCO format dataset, you can train other MOT algorithms without separating detection and tracking such as [FairMOT](https://arxiv.org/abs/2004.01888) on MOT format dataset. In addition, You can easily change the detector to other algorithms such as YOLO7 to leverage your existing object detection algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Convert SageMaker Ground Truth annotation into MOT Challenge annotation\n",
    "\n",
    "`sm_gt_uri` is the label data from SageMaker Ground Truth, and `data_uri` is the video frames from the original video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix = f\"{prefix}/sample-data\"\n",
    "\n",
    "sm_gt_uri = f\"s3://{s3_bucket}/{prefix}/mot-bytetrack-sample/\" # annotation data\n",
    "mot_uri = f\"s3://{s3_bucket}/{prefix}/outputs-mot/\" # output data with MOT format\n",
    "data_uri = f\"s3://{s3_bucket}/{data_prefix}\" # Video frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, if your input data for Tracking task is video file, SageMaker Ground Truth can extract frames from video files, and save these frames in the same directory as video file with the folder name of video name. In the ground truth convet processing, we need to copy this input directory into the instance of SageMaker Processing, which doesn't allow file and folder has the same name in the same directory, therefore we need to delete the original video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "def remove_video_from_s3():\n",
    "    ContinuationToken = None\n",
    "    obj_list = []\n",
    "    while True:\n",
    "        if ContinuationToken:\n",
    "            response = s3_client.list_objects_v2(\n",
    "                Bucket=s3_bucket,\n",
    "                Prefix=data_prefix,\n",
    "                ContinuationToken=ContinuationToken\n",
    "            )\n",
    "        else:\n",
    "            response = s3_client.list_objects_v2(\n",
    "                Bucket=s3_bucket,\n",
    "                Prefix=data_prefix\n",
    "            )\n",
    "        obj_list += response['Contents']\n",
    "\n",
    "        if 'NextContinuationToken' in response:\n",
    "            ContinuationToken = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    for content in obj_list:\n",
    "        if content['Key'].endswith('.mp4'):\n",
    "            s3_client.delete_object(Bucket=s3_bucket, Key=content['Key'])\n",
    "\n",
    "remove_video_from_s3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a SageMaker Processing job to convert annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_job_name = <your labeling job name>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"gt_processing/convert_gt.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=sm_gt_uri, destination=\"/opt/ml/processing/gt\"),\n",
    "        ProcessingInput(source=data_uri, destination=\"/opt/ml/processing/input\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(source=\"/opt/ml/processing/output\", destination=mot_uri)\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--train-ratio\", \"0.6\",\n",
    "        \"--val-ratio\", \"0.2\",\n",
    "        \"--test-ratio\", \"0.2\",\n",
    "        \"--labeling-job-name\", labeling_job_name\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we finish this task, a MOT17 annotation dataset will be created in the defined S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Convert MOT annotation into MSCOCO annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ByteTrack uses [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) to do detection task and then run tracking, and YOLOX uses the MSCOCO annotation dataset to train a model. Therefore we need to convert MOT annotation into MSCOCO annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_uri = f\"s3://{s3_bucket}/{data_prefix}/{dataset_name}/\" # Output data with COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot2coco_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "mot2coco_processor.run(\n",
    "    code=\"gt_processing/mot_to_coco.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=mot_uri, destination=\"/opt/ml/processing/mot\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(source=\"/opt/ml/processing/coco\", destination=coco_uri)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build and push SageMaker training image\n",
    "We use the implementation of [ByteTrack](https://github.com/ifzhang/ByteTrack) to create our own container, and push the image to [Amazon ECR](https://aws.amazon.com/ecr/). For more details about how to use BYOC on SageMaker, please refer to [Adapting your own training container](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-training-container.html).\n",
    "\n",
    "### Docker Environment Preparation\n",
    "Because the volume size of container may be larger than the available size in root directory of the notebook instance, we need to put the directory of docker data into the ```/home/ec2-user/SageMaker/docker``` directory.\n",
    "\n",
    "By default, the root directory of docker is set as ```/var/lib/docker/```. We need to change the directory of docker to ```/home/ec2-user/SageMaker/docker```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /etc/docker/daemon.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ./prepare-docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training image for ByteTrack\n",
    "Use script [`./container/build_tools/build_and_push.sh`](./container-dp/build_tools/build_and_push.sh) to build and push the ByteTrack training image to [Amazon ECR](https://aws.amazon.com/ecr/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat ./container-{version_name}/build_tools/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your *AWS region* as argument, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!bash ./container-train/build_tools/build_and_push.sh {region}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define SageMaker Data Channels\n",
    "In this step, we define SageMaker `train` data channel.\n",
    "\n",
    "Go to [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX/tree/0.1.0), and download a pretrained model (YOLOX-x) from Standard Models. Then upload pretrained model to S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model_s3uri = f's3://{s3_bucket}/{prefix}/pretrained-models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_x.pth\n",
    "!aws s3 cp yolox_x.pth $pretrain_model_s3uri/yolox_x.pth\n",
    "!rm yolox_x.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "mot_input = TrainingInput(s3_data=coco_uri, \n",
    "                            distribution=\"FullyReplicated\", \n",
    "                            s3_data_type='S3Prefix', \n",
    "                            input_mode='File')\n",
    "\n",
    "pretrain_input = TrainingInput(s3_data=pretrain_model_s3uri, \n",
    "                            distribution=\"FullyReplicated\", \n",
    "                            s3_data_type='S3Prefix', \n",
    "                            input_mode='File')\n",
    "\n",
    "data_channels = {'mot': mot_input, 'pretrain': pretrain_input}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the model output location in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = f's3://{s3_bucket}/{prefix}/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Hyper-parameters\n",
    "In this step, we define the hyper-parameters for Training ByteTrack. Jump to [8.Hyperparameter Tuning](#hyperparametertuning) if you want to run hyperparameter tuning job.\n",
    "\n",
    "<table align='left'>\n",
    "    <caption>ByteTrack Hyper-parameters</caption>\n",
    "    <tr>\n",
    "    <th style=\"text-align:center\">Hyper-parameter</th>\n",
    "    <th style=\"text-align:center\">Description</th>\n",
    "    <th style=\"text-align:center\">Default</th>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td style=\"text-align:center\">fp16</td>\n",
    "        <td style=\"text-align:left\">Adopting mix precision training</td>\n",
    "        <td style=\"text-align:center\">0 or 1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">batch_size</td>\n",
    "        <td style=\"text-align:left\">Batch size, should be larger than number_gpu by 2</td>\n",
    "        <td style=\"text-align:center\">24</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">dataset_name</td>\n",
    "        <td style=\"text-align:left\">Assume there are several datasets, choose one dataset you want to train</td>\n",
    "        <td style=\"text-align:center\">'mot'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">occupy</td>\n",
    "        <td style=\"text-align:left\">occupy GPU memory first for training, true by default.</td>\n",
    "        <td style=\"text-align:center\">1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">pretrained_model</td>\n",
    "        <td style=\"text-align:left\">Pretrained model we want to use</td>\n",
    "        <td style=\"text-align:center\">`yolox_x.pth`</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">num_classes</td>\n",
    "        <td style=\"text-align:left\">number of classes</td>\n",
    "        <td style=\"text-align:center\">1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">depth</td>\n",
    "        <td style=\"text-align:left\">depth</td>\n",
    "        <td style=\"text-align:center\">1.33</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">width</td>\n",
    "        <td style=\"text-align:left\">width</td>\n",
    "        <td style=\"text-align:center\">1.25</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">input_size_h</td>\n",
    "        <td style=\"text-align:left\">height in input size</td>\n",
    "        <td style=\"text-align:center\">800</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">input_size_w</td>\n",
    "        <td style=\"text-align:left\">width in input size</td>\n",
    "        <td style=\"text-align:center\">1440</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">test_size_h</td>\n",
    "        <td style=\"text-align:left\">height in test size</td>\n",
    "        <td style=\"text-align:center\">800</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">test_size_w</td>\n",
    "        <td style=\"text-align:left\">width in test size</td>\n",
    "        <td style=\"text-align:center\">1440</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">random_size_h</td>\n",
    "        <td style=\"text-align:left\">height in random size</td>\n",
    "        <td style=\"text-align:center\">18</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">random_size_w</td>\n",
    "        <td style=\"text-align:left\">width in random size</td>\n",
    "        <td style=\"text-align:center\">32</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">max_epoch</td>\n",
    "        <td style=\"text-align:left\">max epoch</td>\n",
    "        <td style=\"text-align:center\">80</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">print_interval</td>\n",
    "        <td style=\"text-align:left\">print_interval</td>\n",
    "        <td style=\"text-align:center\">20</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">eval_interval</td>\n",
    "        <td style=\"text-align:left\">eval_interval</td>\n",
    "        <td style=\"text-align:center\">5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">test_conf</td>\n",
    "        <td style=\"text-align:left\">confidence threshold ranging from 0 to 1</td>\n",
    "        <td style=\"text-align:center\">0.001</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">nmsthre</td>\n",
    "        <td style=\"text-align:left\">IoU threshold of non-max supression ranging from 0 to 1</td>\n",
    "        <td style=\"text-align:center\">0.7</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">basic_lr_per_img</td>\n",
    "        <td style=\"text-align:left\">use a learning rate of lr×BatchSize/64, with a initial lr = 0.01 and the cosine lr schedule</td>\n",
    "        <td style=\"text-align:center\">0.001/64.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">no_aug_epochs</td>\n",
    "        <td style=\"text-align:left\">no_aug_epochs</td>\n",
    "        <td style=\"text-align:center\">10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">warmup_epochs</td>\n",
    "        <td style=\"text-align:left\">warmup_epochs</td>\n",
    "        <td style=\"text-align:center\">1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">infer_device</td>\n",
    "        <td style=\"text-align:left\">device type for inference</td>\n",
    "        <td style=\"text-align:center\">'cuda'</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "                    \"batch_size\": 24,\n",
    "                    \"max_epoch\": 30,\n",
    "                    \"val_intervals\": 1,\n",
    "                    \"pretrained_model\": \"yolox_x.pth\",\n",
    "                    \"fp16\": 0,\n",
    "                    \"infer_device\": \"cuda\"\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Training Metrics\n",
    "Next, we define the regular expressions that SageMaker uses to extract algorithm metrics from training logs and send them to [AWS CloudWatch metrics](https://docs.aws.amazon.com/en_pv/AmazonCloudWatch/latest/monitoring/working_with_metrics.html). These algorithm metrics are visualized in SageMaker console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "            {\n",
    "                \"Name\": \"total_loss\",\n",
    "                \"Regex\": \"total_loss: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"iou_loss\",\n",
    "                \"Regex\": \"iou_loss: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"l1_loss\",\n",
    "                \"Regex\": \"l1_loss: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"conf_loss\",\n",
    "                \"Regex\": \"conf_loss: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"cls_loss\",\n",
    "                \"Regex\": \"cls_loss: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"lr\",\n",
    "                \"Regex\": \"lr: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.50:0.95 | area=all | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.50:0.95 \\| area= all \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.50 | area=all | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.50 \\| area= all \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.75 | area=all | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.75 \\| area= all \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.50:0.95 | area=small | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.50:0.95 \\| area= small \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.50:0.95 | area=medium | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.50:0.95 \\| area=medium \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.50:0.95 | area=large | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.50:0.95 \\| area= large \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=all | maxDets=1)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area= all \\| maxDets= 1 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=all | maxDets=10)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area= all \\| maxDets= 10 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=all | maxDets=100)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area= all \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=small | maxDets=100)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area= small \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=medium | maxDets=100)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area=medium \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=large | maxDets=100)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area= large \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define SageMaker Training Job\n",
    "\n",
    "Next, we use SageMaker [Estimator](https://sagemaker.readthedocs.io/en/stable/estimators.html) API to define a SageMaker Training Job.\n",
    "\n",
    "Multi-GPU instance is not required in this solution, you can choose any other GPU instance for training, <span style=\"color:red\">note that you need to adjust batch size based on the GPU memory to avoid the error of out of memory<span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "bytetrack_estimator = Estimator(image_uri=training_image,\n",
    "                                role=role, \n",
    "                                instance_count=1,\n",
    "                                instance_type='ml.p3.16xlarge',\n",
    "                                volume_size = 100,\n",
    "                                max_run = 40000,\n",
    "                                output_path=s3_output_location,\n",
    "                                sagemaker_session=sagemaker_session, \n",
    "                                hyperparameters = hyperparameters,\n",
    "                                metric_definitions = metric_definitions,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we launch the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "job_name=f'bytetrack-{version_name}-{int(time.time())}'\n",
    "print(f\"Launching Training Job: {job_name}\")\n",
    "\n",
    "# set wait=True below if you want to print logs in cell output\n",
    "bytetrack_estimator.fit(inputs=data_channels, job_name=job_name, logs=\"All\", wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the metrics of the training job in the `Training Job` console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "    f'<b><a href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/jobs/{job_name}\">Check the status of training job</a></b>'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once above training job completed**, we store the S3 URI of the model artifact in IPython’s database as a variable. This variable will be used to serve model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_uri = bytetrack_estimator.model_data\n",
    "%store s3_model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hyperparametertuning'></a>\n",
    "## 8.Hyperparameter Tuning\n",
    "In this step, we define and launch Hyperparameter tuning job. `MaxParallelTrainingJobs` should be <span style=\"color:red;\">**equal or less than the limit of training job instance**</span>. We choose `depth` and `width` for tuning and set `total_loss` to the objective metric. \n",
    "\n",
    "As [Best Practices for Hyperparameter Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-considerations.html) suggests, a tuning job improves only through successive rounds of experiments. Therefore, smaller `MaxParallelTrainingJobs` and larger `MaxNumberOfTrainingJobs` may lead to a better result. When `MaxParallelTrainingJobs` is equal to `MaxNumberOfTrainingJobs`, searching strategy will become `Random Search` even setting it as `Bayesian Search`. In this demonstration, we set `MaxParallelTrainingJobs` to 1.\n",
    "\n",
    "For `MaxNumberOfTrainingJobs`, setting a larger `MaxNumberOfTrainingJobs` cat get the better result, but it takes a longer time. We set `MaxNumberOfTrainingJobs` to the small value 3 to show how SageMaker Hyperparameter works. When you train a model on your own dataset, we recommend to set `MaxNumberOfTrainingJobs` to a larger value.\n",
    "\n",
    "For more details on Hyperparameter tuning with SageMaker, you can reference [How Hyperparameter Tuning Works](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import gmtime, strftime\n",
    "\n",
    "tuning_job_name = f'bytetrack-tuningjob-' + strftime(\"%d%H%M%S\", gmtime())\n",
    "\n",
    "print(tuning_job_name)\n",
    "\n",
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"Name\": \"basic_lr_per_img\",\n",
    "          \"MaxValue\": \"0.000016625\",\n",
    "          \"MinValue\": \"0.000013625\",\n",
    "          \"ScalingType\": \"Auto\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 3,\n",
    "      \"MaxParallelTrainingJobs\": 1\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"total_loss\",\n",
    "      \"Type\": \"Minimize\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "      \"MetricDefinitions\": [\n",
    "            {\n",
    "                \"Name\": \"total_loss\",\n",
    "                \"Regex\": \"total_loss: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"iou_loss\",\n",
    "                \"Regex\": \"iou_loss: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"l1_loss\",\n",
    "                \"Regex\": \"l1_loss: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"conf_loss\",\n",
    "                \"Regex\": \"conf_loss: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"cls_loss\",\n",
    "                \"Regex\": \"cls_loss: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"lr\",\n",
    "                \"Regex\": \"lr: (.*?),\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.50:0.95 | area=all | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.50:0.95 \\| area= all \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.50 | area=all | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.50 \\| area= all \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.75 | area=all | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.75 \\| area= all \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.50:0.95 | area=small | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.50:0.95 \\| area= small \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.50:0.95 | area=medium | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.50:0.95 \\| area=medium \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AP(IoU=0.50:0.95 | area=large | maxDets=100)\",\n",
    "                \"Regex\": \"AP\\) @\\[ IoU=0.50:0.95 \\| area= large \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=all | maxDets=1)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area= all \\| maxDets= 1 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=all | maxDets=10)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area= all \\| maxDets= 10 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=all | maxDets=100)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area= all \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=small | maxDets=100)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area= small \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=medium | maxDets=100)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area=medium \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"AR(IoU=0.50:0.95 | area=large | maxDets=100)\",\n",
    "                \"Regex\": \"AR\\) @\\[ IoU=0.50:0.95 \\| area= large \\| maxDets=100 \\] = ([0-9\\.]+)\"\n",
    "            }\n",
    "      ],\n",
    "      \"TrainingImage\": training_image,\n",
    "      \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3train,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "      \"S3OutputPath\": s3_output_location\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "      \"InstanceCount\": 1,\n",
    "      \"InstanceType\": \"ml.p3.16xlarge\",\n",
    "      \"VolumeSizeInGB\": 100\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "        \"batch_size\": \"24\",\n",
    "        \"max_epoch\": \"10\",\n",
    "        \"val_intervals\": \"1\",\n",
    "        \"pretrained_model\": \"yolox_x.pth\",\n",
    "        \"fp16\": \"0\",\n",
    "        \"infer_device\": \"cuda\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "      \"MaxRuntimeInSeconds\": 72000\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we launch the defined hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient = boto3.client('sagemaker')\n",
    "smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                               HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                               TrainingJobDefinition = training_job_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the hyperparamter tuning job in the `Hyperparameter tuning jobs`console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "    f'<b><a href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/hyper-tuning-jobs/{tuning_job_name}\">Check hyperparameter tuning job</a></b>'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
