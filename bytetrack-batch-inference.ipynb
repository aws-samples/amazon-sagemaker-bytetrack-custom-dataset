{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ByteTrack Batch Inference in Amazon SageMaker\n",
    "\n",
    "In this notebook, we will walk through the batch inference with the FairMOT model. Because [SageMaker Batch Transform only can run the batch transform job finishing within 600 seconds](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html), we use SageMaker processing to run the batch inference.\n",
    "\n",
    "## 1. SageMaker Initialization \n",
    "First we upgrade SageMaker to the latest version. If your notebook is already using latest Sagemaker 2.x API, you may skip the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade pip\n",
    "! python3 -m pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "print(f'AWS account:{account}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "aws_region = session.region_name\n",
    "print(f\"AWS region:{aws_region}\")\n",
    "\n",
    "container_name = \"container-batch-inference\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build and Push Amazon SageMaker Serving Container Images\n",
    "\n",
    "For this step, the [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) attached to this notebook instance needs full access to [Amazon ECR service](https://aws.amazon.com/ecr/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Docker Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the volume size of container may exceed the available size in the root directory of the notebook instance, we need to put the directory of docker data into the ```/home/ec2-user/SageMaker/docker``` directory.\n",
    "\n",
    "By default, the root directory of docker is set as ```/var/lib/docker/```. We need to change the directory of docker to ```/home/ec2-user/SageMaker/docker```. You can skip this step if you have done docker environment preparation in [`bytetrack-training.ipynb`](bytetrack-training.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ./prepare-docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build and Push ByteTrack Serving Container Image\n",
    "\n",
    "Use [`./container-batch-inference/build_tools/build_and_push.sh`](./container-batch-inference/build_tools/build_and_push.sh) script to build and push the [FairMOT](https://github.com/ifzhang/FairMOT) <b>batch inference</b> container image to Amazon ECR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat ./{container_name}/build_tools/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your *AWS region* as argument, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "! ./{container_name}/build_tools/build_and_push.sh {aws_region}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image = f\"{account}.dkr.ecr.{aws_region}.amazonaws.com/bytetrack-sagemaker:pytorch1.12.1-batch-inference\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Batch Inference\n",
    "\n",
    "Create an instance of SageMaker Processing with the built container above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_processor = ScriptProcessor(\n",
    "                image_uri=inference_image,\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.p3.2xlarge',\n",
    "                command=['python3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the S3 URIs for the test data, the trained model and the result data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"\" # your-s3-bucket-name\n",
    "\n",
    "# Restore the s3 uri of the trained model\n",
    "%store -r s3_model_uri\n",
    "\n",
    "s3_input = f\"s3://{bucket_name}/sm-bytetrack/batch-inference/test-videos\"\n",
    "s3_output = f\"s3://{bucket_name}/sm-bytetrack/batch-inference/output/batch-inference\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [MOT16-03](https://motchallenge.net/sequenceVideos/MOT16-04-raw.webm) from MOT challenge to test the batch inference. First, we download the video to the notebook instance and then upload it to the defined s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ifzhang/FairMOT/master/videos/MOT16-03.mp4\n",
    "!aws s3 cp MOT16-03.mp4 {s3_input}/MOT16-03.mp4\n",
    "!rm MOT16-03.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the sagemaker processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "script_processor.run(\n",
    "    code='./container-batch-inference/predict.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(source=s3_input, destination=\"/opt/ml/processing/input\"),\n",
    "        ProcessingInput(source=s3_model_uri, destination=\"/opt/ml/processing/model\"),\n",
    "    ], \n",
    "    outputs=[\n",
    "        ProcessingOutput(source='/opt/ml/processing/output', destination=s3_output),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the result saved in `s3://{bucket-name}/bytetrack/sagemaker/output/batch-inference`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert inference result into label data compatible with SageMaker Ground Truth \n",
    "We can accelerate data labeling with SageMaker Ground Truth by leveraging the inference results on new dataset from batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"track-label-job-221207\"\n",
    "\n",
    "mot_s3uri = f\"s3://{bucket_name}/bytetrack/sagemaker/output/batch-inference/track_vis/2022_12_07_03_43_53/mot/\"\n",
    "gt_s3uri = f\"s3://{bucket_name}/bytetrack/sagemaker/sg-ground-truth/{job_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"gt_processing/mot_to_gt.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=mot_s3uri, destination=\"/opt/ml/processing/input/data\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(source=\"/opt/ml/processing/output\", destination=gt_s3uri)\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--input-s3uri\", f\"{bucket_name}/bytetrack/sagemaker/sg-ground-truth/{job_name}/inputs\",\n",
    "        \"--output-s3uri\", f\"{bucket_name}/bytetrack/sagemaker/sg-ground-truth/{job_name}\",\n",
    "        \"--job_name\", job_name\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
